---
title: "TV Watching Habits Study"
author: "Cameron Bell, Alyssa Eisenberg, Sarah Cha"
date: "4/26/2018"
output: pdf_document
abstract: "Television is consumed through a growing number of channels today (cable, streaming networks like Hulu, Amazon, Netflix, and Youtube) while increasingly the mould for what looks like television is becoming more fluid (e.g. video clips on Youtube). We suspect some individuals don't actually realize how much television they're watching on a daily basis and we set out to understand how social pressures around television watching would alter or impact individual behavior. In our study, we randomly assigned treatment emails to half of the participants that highlights how much their peers are watching TV on average. We found consistent negative point estimates of our treatment effect of 0.2-1hr/week, but did not obtain statistical significance for these results."
---

```{r setup, include=FALSE}
# Tidy up any code printed in pdf
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

```{r, include=FALSE}
# Load libraries
library(data.table) 
library(lmtest)
library(ggplot2)
library(tidyr)
library(dplyr)
library(reshape)
library(corrplot)
library(stargazer)
library(gridExtra)
library(data.table)
``` 

##Introduction

Nielsen Total Audience report for the fourth quarter of 2016 discussed the steady trend of decline in live TV watching alongside increased use of smartphones to view video and the increasing penetration of subscription video on demand. While live viewing decreased to 4 hours and 23 minutes a day among adults 18 years and up (from 4 hours and 27 minutes in the prior year), smartphone usage rose to 2 hours and 32 minutes a day from 1 hour and 15 minutes a year ago. Needless to say, individuals are consuming video content in more formats today and given the rate at which online and on-demand content is growing, consumers can access an endless amount of content right at their fingertips. This points to a trend of increased video/screen watching and could mean that consumers are watching more TV than they actually realize or desire. 

In other contexts, it has been found that providing peer comparisons can cause changes in people's behavior. For example, in the Opower study by Ian Ayres, Sophie Raseman, and Alice Shih, they find that providing mailed peer feedback reports resulted in reductions in energy consumption. We are interested in whether a similar mechanism can be applied to the new context of TV watching habits. 

We set up an experimental study where we randomly assigned treatment emails to half of the participants that summarizes how much their peers are watching television on average. We wanted to understand if social/peer pressure has the ability to impact how much television a person watches (e.g. "Wow, my peers are watching only 2 hours of TV a week and I'm watching at least 10. Shame on me."). 

We begin by discussing the high level experimental design we developed to test whether social pressures/influence can actually impact individual behavior as it relates to TV watching, including changes based on an initial pilot study. Next, we discuss the details of our real experiment - the participants, the nature and frequency of the treatment, and our outcome measures. Then, we complete a range of statistical analyses suggesting a not statistically signficant negative treatment effect of 0.2-1 hr/week, taking into account attrition in the experiment. We conclude by discussing the implications for generalizability to a broader population at large and future potential avenues for exploration.

##Experimental Design

Our aim was to develop a means for studying the influence of social pressures on TV watching, recognizing that the means by which Americans consume TV is very diverse today (cable, online video streaming, YouTube, etc) and what people classify as television can vary quite a bit (e.g. television shows, news, sports, movies, YouTube clips, etc). We hypothesized that when we carefully defined what we classify as "television" watching and participants could compare apples to apples their consumption vs. peers, they'd be impacted if their peers were watching less than they were. We also hypothesized that we'd see heterogeneous treatment effect from our participants based on amount of TV watched prior to experiment (ie. those in 1st quartile are not likely to reduce their amount of TV and may actually see an increase instead).  

We decided on surveys as the means by which we would collect all the necessary information including outcome measures from our participants. Our experimental design follows a difference in difference approach. Our experiment consisted of two surveys - one pre and one post treatment. In the pre-survey,  we would measure pre-survey TV behavior for all participants along with what we thought could be key covariates of interest (age, gender, marital/employment status, etc). Then we would randomly assign individuals to treatment vs. control, where the individuals in the treatment group would be aware of how much their peers were watching. The post-survey measures TV behavior during the experiment. This allows us to compare the hours of TV watched between treatment and control group for causal estimate conditioning on pre-experiment TV watching measure as a covariate.

```{r echo=FALSE, fig.cap="Experimental Design", out.width = '100%'}
knitr::include_graphics("design.png")
```

In designing our study, we were very mindful of how to maximize participation to get a large enough sample size, get accurate reportings, and how we could extract the most information in the least intrusive way. On the latter, conversations with our peers made us realize that there was clearly a balance between incentives for participants and the time commitment involved. We offered a drawing for 5 Amazon gift cards for $20 each as a reward for participating, with the stipulation that participants had to complete a 3 minute pre-survey and a 1 minute post-survey in order to be entered into the drawing. 

**Pilot Study**

We implemented a pilot study with 9 individuals to test our initial design framework and help identify potential problems and inform our power analysis. The pilot took place over the course of 1 week (Feb 26 - Mar 4) where friends and family answered a pre-survey, were randomly assigned to control or treatment, treatment group was sent an email about peer usage, and then everyone filled out a post-survey on their TV consumption. See the Appendix for full questionaires.

*Learnings and adjustments in the experimental design:*

1. Attrition is likely to be high. Of the 9 close family and friends that we targeted for the survey, we ended up with full observation sets for only 7 of 9. Of course the short observation time had an impact in the case of the pilot. To reduce attrition as much as possible in order to maximize the inference ability of the experiment, we incorporated more time for individuals to fill out both the pre- and post-survey and planned to send several reminder emails emphasizing the Amazon gift card drawing for completing participation in the study.

2. The quality of the data coming out of the Chrome extension reading was low - 3 of 7 individuals had 0 or NA output even though they indicated they watched TV during that time. This emphasized that we needed to incorporate more optional language around the Chrome Netflix extension. We didn't want to make installation of that extension and its output such a strong requirement with the fear that it would reduce our subject pool too severely. 

3. The data from the self-reported average amount of TV watched did not have much variability. This was in part due to the low number of participants, but also because the response options were a limited set of ranges of hours. This made the treatment less effective in showing variation, so we changed the question about pre-experiment self-reported TV to have participants enter their own value instead of selecting a radio button.

4. We emailed the pre- and post-surveys out via Qualtrics with the sender name as "UCBerkeley TV Habits Study" but the treatment email came from Alyssa's gmail, raising the question if this created confusion and/or potential for individuals to ignore Alyssa's emails (non-compliance). To resolve this, we created a gmail account with the same title as the Qualtric email sender which is "UCBerkeley TV Habits Study."

**Calculating Power**

The pilot also provided reference information for a power calculation that would help us understand what level of treatment effect we would be able to accurately identify. We ran a simulation regressing outcome TV on treatment and using the pre-experiment amount of TV watched as a covariate to reduce our SE and increase the power, and checked the proportion that found a statistically significant coefficient for treatment. We made the following simplifying assumptions for our simulation:

- set mean and variance for self-reported pre-experiment TV hours watched
- each individuals potential outcomes to control are normally distributed with mean of that individuals TV prior to experiment, and a SD value provided as an input
- treatment effect is constant for everyone

Since the pilot was so short, we did not get a good estimate of the treatment effect and we have no data for individual SD for TV watched. Instead, we run across a variety of treatment effects and SD values and check the power for that combination of parameters.

First, we ran this analysis using the pilot mean and variance for pre-experiment hours of TV (7.6 and 5.25 respectively). We found that for 100 participants and an individual SD of 2.5, we would need a treatment effect of -1.5 hrs/week to have 80% power. We updated and re-ran this analysis after gathering the pre-survey data for the real experiment, leading to the chart below. Here we only use 80 participants, and the mean and variance for pre-experiment hours of TV from the pre-survey (8.6 and 6.25). If the individual SD was about half that of the overall population (3), we would need a treatment effect of -2 hrs/week to have 80% power.

```{r fig.height= 3.5, echo=FALSE}
individual_simulation_covariate <- function(
  units, mean_control, sd_control, tau, sd_individual) {
  ## units is our sample size
  ## mean_control is the mean TV watched prior to experiment
  ## sd_control is the standard deviation in TV watched prior to experiment
  ## tau is the treatment effect (change in amount of TV for treatment)
  ## sd_individual is the SD for an individuals weekly TV watched with no treatment
  
  ## Assume simple randomization among all units with 50% probability of treatment
  urn <- c('treat', 'control')
  d <- data.table(id = 1:units)
  d[ , condition := sample(urn, size = .N, replace = TRUE)]
  d[ , treated := as.numeric(condition == "treat")]
  
  ## Assumes a normal distribution of pre-experiment avg weekly TV
  ## Mean and SD are supplied to function
  ## Assumes each individual has a normal distribution centered on their pre-experiment value
  ## Assumes a constant treatment effect across all participants
  d[ , preTV := pmax(rep(0, units),rnorm(units,mean=mean_control, sd = sd_control))]
  d[ , Y0 := pmax(rep(0, units),rnorm(units,mean=preTV, sd = sd_individual))]
  d[ , Y1 := pmax(rep(0, units),Y0 + tau)]
  d[condition == 'control' , outcome := Y0]
  d[condition == 'treat' , outcome := Y1]

  ## Run a regression including preTV as a covariate
  model <- lm(outcome ~ treated + preTV, data=d)
  
  res <- list(
  'pvalue' = coeftest(model)[2,4],
  'tau' = tau, 
  'sd_individual' = sd_individual,
  'baseline_n'    = d[condition == 'control', .N],
  'alt_n'         = d[condition == 'treat', .N],
  'df_n'          = d[ , .N],
  'ate'           = unname(model$coefficients[2])
  )

  return(res)
}

# Allow treatment effect and sd_individual to vary and run many simulated experiments
# Note this takes over ten minutes to run
moving_tau <- seq(from=-1, to= -4, by=-.1)
moving_sd_individual <- seq(from=1, 5, by=0.2)

results <- replicate(50000, 
                     individual_simulation_covariate(80, 8.6, 6.25, sample(moving_tau, size = 1), sample(moving_sd_individual, size=1)),
                     simplify = FALSE)
results <- rbindlist(results)
results$sig <- as.numeric(results$pvalue<0.05)

# Calculate power at each treatment effect level and plot them
results_agg <- aggregate(sig ~ tau + sd_individual, FUN=mean, data=results)
results_agg$over80 <- factor(as.numeric(results_agg$sig>=0.8))

ggplot(results_agg, aes(x=tau, y=sd_individual)) +
  geom_raster(aes(fill = sig)) +  scale_fill_gradientn(colours = rev(terrain.colors(10))) + 
  ggtitle("Power Level by Treatment Effect Size and Individual SD")

```

##The Actual Experiment  

**Subjects**  

In an ideal scenario,  we would survey a large sample of the US adult population in order to get a representative sample for our experiment. However, with the key constraints being time and resources (we have only about 10 weeks and $500), we implemented a much more limited study where we, ourselves, were tasked with finding our respondents by word of mouth and social media.

We recruited on college campuses and posted on social media to get participants. We chose to limit participants to one per household to avoid problems with non-interference. Even though we used different methods of outreach including word of mouth, social media, and email, we used standardized pitch with key talking points. This ensured that all three of us were giving out the same details about the experiment and, more importantly, that none of us provided too much detail on what we were aiming to do/analyze to avoid participants skewing their answers. See appendix for the standardized pitch used.

Initially we took down email addresses of all interested participants. There were 136 email addresses of potential participants of which 42 came from BYU, 38 from social media websites like Facebook and Instagram, 37 from UCBerkeley Slack, and 19 friends of Alyssa, Cameron, and Sarah.

Everyone on that list was emailed a pre-survey that they had to fill out to be a part of the actual experiment. Just as with the pilot, the pre-survey asked all participants about their initial TV watching habits and covariates at the beginning of the experiment. Useful potential covariates included age, martial status, and pre-existing TV watching habits. There was about 40% drop out rate with only 81 of the 136 individuals filling out the pre-survey by the deadline for the experiment (another 2 completed the survey after treatment/control was already randomized). For the full flow of participants through the experiment, please see the Appendix.

**Random assignment of treatment and controls**

In terms of randomization once we had our set list of participants from the pre-survey, we used a simple random assignment to get half of our subjects into control and the other half in treatment in an effort to prevent against any selection bias. Those assigned to control received an email thanking them for their participation, while those in treatment received the same email with a chart comparing their amount of TV watched with their peers (see Appendix for examples).

For a check on our randomization before sending these emails, we did a covariate balance check and found that the randomization had worked. There were no statistically significant differences between the control and treatment group on key variables. Also, the F-test of joint significance on a regression of treatment assignment on all our key covariates did not return anything of significance (p-value of 67%). For the full model results, see the Appendix.

```{r echo=FALSE}
# Load the pre-survey data and run cleaning steps
data <- read.csv("UCBerkeley_TV_Habits_Study_Pre-Survey_March_2018_13.01_CLEAN.csv.csv")

# Remove extraneous columns and rename remaining columns
data <- data [,c(12,18:36)]
colnames(data) <- c("linkedEmail", "enteredEmail", "gender", "age", "region", "employment", "maritalStatus", "children", "hoursTV", "binge", "primaryChannel", "allMethods", "moreTimeThanWanted", "watchAlone", "shareProfile", "netflixDays", "netflixHours", "netflixMin", "netflixAccountAndChrome", "source")

# Generate factors for all multiple choice columns
for (i in c(3:8,10:11,13:15,19)){
  data[,i] <- factor(data[,i])
}
# Label answers
levels(data$gender)=c("male", "female")
levels(data$age)=c("21-","22-34","35-44","45-54","55-64","65+")[as.numeric(levels(data$age))]
levels(data$region)=c("midwest", "northeast", "southeast","southwest","west","outsideUS")[as.numeric(levels(data$region))]
levels(data$employment)=c("full","part","looking","unemployed","student","retired","homemaker","self","unable")[as.numeric(levels(data$employment))]
levels(data$maritalStatus)=c("single","married","widowed","divorced","Separated")[as.numeric(levels(data$maritalStatus))]
levels(data$children)=c("yes","no")[as.numeric(levels(data$children))]
levels(data$binge)=c("once a week","once a month","once every couple months","once a year","no")[as.numeric(levels(data$binge))]
levels(data$primaryChannel)=c("netflix","HBO","hulu","amazon","youtube","cable","other")[as.numeric(levels(data$primaryChannel))]
levels(data$moreTimeThanWanted)=c("once a year","couple times a year","once a month","couple times a month","once a week")[as.numeric(levels(data$moreTimeThanWanted))]
levels(data$watchAlone)=c("alone","withOthers")[as.numeric(levels(data$watchAlone))]
levels(data$shareProfile)=c("yes","no")[as.numeric(levels(data$shareProfile))]
levels(data$netflixAccountAndChrome)=c("both", "noNetflix","neither", "noChrome")


# Simple random assignment (treat=1 mean it is in the treatment group)
# set seed so that results of random process are reproducible
set.seed(569320)
data$treat <- sample(c(1,0), size = nrow(data), replace = TRUE)
```

```{r echo=FALSE}
# Covariate balance check two ways

# Statistical F-test
cov_balance_check1 <- lm(treat ~ gender + age + region + employment + maritalStatus + children + hoursTV + binge + primaryChannel + moreTimeThanWanted + watchAlone + shareProfile + source, data=data)


# Additionally, examine levels of key covariates by treatment/control
se_diff_means <- function(treatment, control) {
  round(sqrt(sd(control)^2/length(control) + sd(treatment)^2/length(treatment)),2)
}

# hours of TV
t1 <- round(mean(data$hoursTV[data$treat==1]),2)
c1 <- round(mean(data$hoursTV[data$treat==0]),2)
diff1 <- t1-c1
se1 <- se_diff_means(data$hoursTV[data$treat==1],data$hoursTV[data$treat==0])

# male
t2 <- round(mean(data$gender[data$treat==1]=="male"),2)
c2 <- round(mean(data$gender[data$treat==0]=="male"),2)
diff2 <- t2-c2
se2 <- se_diff_means(as.numeric(data$gender[data$treat==1]=="male"),as.numeric(data$gender[data$treat==0]=="male"))

# marital status = married
t3 <- round(mean(data$maritalStatus[data$treat==1]=="married"),2)
c3 <- round(mean(data$maritalStatus[data$treat==0]=="married"),2)
diff3 <- t3-c3
se3 <- se_diff_means(as.numeric(data$maritalStatus[data$treat==1]=="married"),as.numeric(data$maritalStatus[data$treat==0]=="married"))

# no children
t4 <- round(mean(data$children[data$treat==1]=="no"),2)
c4 <- round(mean(data$children[data$treat==0]=="no"),2)
diff4 <- t4-c4
se4 <- se_diff_means(as.numeric(data$children[data$treat==1]=="no"),as.numeric(data$children[data$treat==0]=="no"))

# moreTimeThanWanted: couple times a year
t5 <- round(mean(data$moreTimeThanWanted[data$treat==1]=="couple times a year"),2)
c5 <- round(mean(data$moreTimeThanWanted[data$treat==0]=="couple times a year"),2)
diff5 <- t5-c5
se5 <- se_diff_means(as.numeric(data$moreTimeThanWanted[data$treat==1]=="couple times a year"),as.numeric(data$moreTimeThanWanted[data$treat==0]=="couple times a year"))

# moreTimeThanWanted: once a month
t6 <- round(mean(data$moreTimeThanWanted[data$treat==1]=="once a month"),2)
c6 <- round(mean(data$moreTimeThanWanted[data$treat==0]=="once a month"),2)
diff6 <- t6-c6
se6 <- se_diff_means(as.numeric(data$moreTimeThanWanted[data$treat==1]=="once a month"),as.numeric(data$moreTimeThanWanted[data$treat==0]=="once a month"))

# moreTimeThanWanted: couple times a month
t7 <- round(mean(data$moreTimeThanWanted[data$treat==1]=="couple times a month"),2)
c7 <- round(mean(data$moreTimeThanWanted[data$treat==0]=="couple times a month"),2)
diff7 <- t7-c7
se7 <- se_diff_means(as.numeric(data$moreTimeThanWanted[data$treat==1]=="couple times a month"),as.numeric(data$moreTimeThanWanted[data$treat==0]=="couple times a month"))

# moreTimeThanWanted: once a week
t8 <- round(mean(data$moreTimeThanWanted[data$treat==1]=="once a week"),2)
c8 <- round(mean(data$moreTimeThanWanted[data$treat==0]=="once a week"),2)
diff8 <- t8-c8
se8 <- se_diff_means(as.numeric(data$moreTimeThanWanted[data$treat==1]=="once a week"),as.numeric(data$moreTimeThanWanted[data$treat==0]=="once a week"))


# Put into a table for display
d <- data.frame(variable = c("hours TV", "male", "married", "no children", "watched more than wanted: couple times a year", "watched more than wanted: once a month", "watched more than wanted: couple times a month", "watched more than wanted: once a week"), 
                control = c(t1, t2, t3, t4, t5, t6, t7, t8), 
                treatment = c(c1, c2, c3, c4, c5, c6, c7, c8),
                diff = c(diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8),
                se = c(se1, se2, se3, se4, se5, se6, se7, se8) )
knitr::kable(d)
```

**Outcome Measures**  

We carefully define in our communication with participants what we consider to be TV consumption (movies, tv shows, or video clips through cable, streaming online, or Youtube videos). Our primary outcome measure asks survey respondents to estimate and record how many hours they watched during the two weeks of the experiment. Please see the histogram below for the distribution of the responses, which is positively skewed with a max of around 80 hours in the two week period.

For our secondary outcome measure, we created a Chrome Netflix extension that would track and display all Netflix usage an individual had during a set period for the individual to then input the numbers directly into the post-survey. The goal was to avoid measurement error from self-reporting due to issues in memory or miscounting. However, only 55% of final survey respondents included this information, and of those only 14% did not share their Netflix profile with another person. Since the extension sums all activity for a Netflix profile for the given period of time, that is how we see extremely high records of TV watched as can be seen in the histogram below (e.g., several people with over 150 hours in a two week period). These issues in the measurement make this data unusable for results analysis.

```{r fig.height=3.5, echo=FALSE, warning=FALSE}
#load the survey data
survey_data <- read.csv("UCBerkeleyTVHabitsStudyPost-Survey_April2018CLEAN.csv")
survey_data$Q3_1 <- as.numeric(survey_data$Q3_1)
survey_data$Q3_2 <- as.numeric(survey_data$Q3_2)
survey_data$Q3_3 <- as.numeric(survey_data$Q3_3)

#first just pull the relevant columns from survey data
cols <- c(12, 18:24)
sub.data <- survey_data[ ,cols]
sub.data$Finished<- 1 #add a variable for if they finished the post-survey
colnames(sub.data)=c("linkedEmail", "post_survey_ID", "reportedhrs_P", "netflixDays_P", "netflixHours_P", "netflixMin_P","Treated", "Finished" )

#join the pre-survey & survey data set using emails
all_data <- merge(data, sub.data, by="linkedEmail", all=TRUE)
all_data$Finished <- ifelse(is.na(all_data$Finished), 0, 1)
all_data$Assigned <- all_data$treat
all_data <- all_data[ ,-21]

#Clean manipulation check data - question asking users whether they remember receiving bar chart in email
#recode Yes = 1, No = 0, Blank/NA = NA
levels(all_data$Treated) <- c("Yes", "No", "")
all_data$Treated <- as.numeric(all_data$Treated)
all_data$Treated[is.na(all_data$Treated)] <- 0
all_data$Treated <- ifelse(all_data$Treated == 3, 1, 0)

#Create first quartile dummy
all_data$first_quartile <- all_data$hoursTV < quantile(all_data$hoursTV, 0.25)

#clean Netflix data
all_data$NFLXreading <- (all_data$netflixDays_P*24) + all_data$netflixHours_P + (all_data$netflixMin_P/60)
all_data$extension <- is.na(as.numeric(all_data$netflixMin)) #indicates whether used extension in pre-survey

#Plot outcome distributions
g1 <- ggplot(all_data[all_data$Finished==1,], aes(reportedhrs_P)) + geom_histogram(bins=30) +
  ggtitle("Hist of Self-Reported Hours")
g2 <- ggplot(all_data[all_data$Finished==1,], aes(NFLXreading)) + geom_histogram(bins=30) +
  ggtitle("Hist of Netflix Hours")
grid.arrange(g1,g2, ncol=2)

#Netflix usage stats
#pct of final surveys that provided Netflix data
pct_NFLX <- length(na.omit(all_data$NFLXreading)) / sum(all_data$Finished==1)
#pct of final surveys providing Netflix data who did not share a Netflix profile
pct_sharing <- length(na.omit(all_data$NFLXreading[all_data$shareProfile=="no"])) /
  length(na.omit(all_data$NFLXreading))
```

##Results

**Attrition**

We witnessed high levels of attrition through out the experiment, ending up with 65 participants completing the post survey from 81 who were assigned into treatment vs. control. From the time the participants were assigned to treatment and control groups, we saw 17% attrition in the treatment group and 23% in the control. 

Beyond examining direct attrition rates, we ran two tests for differential attrition and found no evidence to suggest that attrition differed based on treatment group. When we regressed completion on treatment group, we found that the coefficient on treatment assignment was not significant with a p-value of .475. Further, an ex-ante covariate balance check fails to reject null of no joint significance of covariates predicting treatment assignment with a p-value of 0.7853 (see Appendix for full results). Given this, we will continue the analysis excluding the subjects who attrited, under the assumption that without differential attrition by treatment assignment we are calculating the ATE of the 'Always-Reporter'. 

```{r echo=FALSE}
#count attrition by assignment
xtabs(~ Finished + Assigned, data=all_data)
```

```{r results='asis', echo=FALSE}
#attrition checks
attrit_by_assigned <- lm(Finished ~ Assigned, data=all_data)
stargazer(attrit_by_assigned, header=F)

attrit_covariate_check <- lm(Assigned ~ gender + age + region + employment + maritalStatus + children + hoursTV + binge + primaryChannel + moreTimeThanWanted + watchAlone + shareProfile + source, data=all_data[all_data$Finished == 1 ,])
```

**Initial Findings**

First, we examine the distributions of the weekly amount of TV watched by treatment assignment and calculate the difference-in-differences estimator by comparing the change in amount of weekly TV watched pre-experiment and during the experiment for the treatment and control groups. This finds an ATE of almost a one hour reduction in weekly TV, but a high standard error indicating that this is not statistically significant.

The boxplot of the distributions show the same story - a slight increase in the median TV watched in the control group, and a slight decrease for the treatment group. However, you can see the wide spread in hours of TV watched contributing to the high standard errors.

```{r echo=FALSE}
# calculate ATE
t9 <- round(mean(all_data$reportedhrs[all_data$Assigned==1 & all_data$Finished==1]/2 -
                   all_data$hoursTV[all_data$Assigned==1 & all_data$Finished==1], na.rm=T),2)
c9 <- round(mean(all_data$reportedhrs[all_data$Assigned==0 & all_data$Finished==1]/2 -
                   all_data$hoursTV[all_data$Assigned==0 & all_data$Finished==1], na.rm=T),2)
diff9 <- t9-c9
se9 <- se_diff_means(as.numeric(all_data$reportedhrs[all_data$Assigned==1 & all_data$Finished==1]/2 -
                   all_data$hoursTV[all_data$Assigned==1 & all_data$Finished==1]), 
              as.numeric(all_data$reportedhrs[all_data$Assigned==0 & all_data$Finished==1]/2 -
                   all_data$hoursTV[all_data$Assigned==0 & all_data$Finished==1]))

#Put into a table for display
outcomes <- data.frame(variable = c("Change in Weekly Hrs TV"),
                       control = c(c9),
                       treatment = c(t9),
                       diff = c(diff9),
                       se = c(se9))
knitr::kable(outcomes, 'latex') 
```

```{r echo=FALSE}
# control and treatment, before and after
#note that we exclude pre-survey responses from people who later attrited
boxplot(all_data$reportedhrs_P[all_data$Assigned==0]/2,
        all_data$hoursTV[all_data$Assigned==0 & all_data$Finished == 1],
        all_data$reportedhrs_P[all_data$Assigned==1]/2,
        all_data$hoursTV[all_data$Assigned==1 & all_data$Finished == 1],
        names=c('After', 'Before', 'After', 'Before'), main='Hours Watched', 
        xlab='Hours of TV Watched per Week', ylab="Control            Treatment",
        ylim=c(0,40), horizontal=T)
                                                                                                       
```

We also examine the results of our manipulation check, which was a question for those in treatment about whether they received an email with a bar chart providing information about their peers TV watching habits. This manipulation check is not a perfect identification of compliers since participants could either have not seen the email (ie, non-complier), or could have seen it and not remembered it (ie, weak treatment effect).

The below boxplot shows the distributions for our treatment participants who did and did not report receiving the email with the treatment bar chart. We see that the distribution for non-compliers here appears different from the control group above since median hours of TV did decrease, but also appears to have a much smaller effect than compliers where the median hours of TV decreased more substantially. This indicates that at least some of the participants had a weak treatment effect as opposed to being true non-compliers.

Still, analyzing this as a measure of non-compliance is interesting as it gives us an upper bound on the CACE. We already know from above that the ATE was -0.94. Our compliance rate was only 43%, leading to a CACE of -2.2 hours/week.

```{r echo=FALSE}
# CACE analysis
ATE <- diff9
alpha <- mean(all_data$Treated[(all_data$Assigned == 1) & (all_data$Finished == 1)])
CACE <- ATE/alpha

#treatment before and after, split by manipulation check
#note that we exclude pre-survey responses from people who later attrited
boxplot(all_data$reportedhrs_P[all_data$Assigned==1 & all_data$Treated==1]/2,
        all_data$hoursTV[all_data$Assigned==1 & all_data$Treated==1 & all_data$Finished == 1],
        all_data$reportedhrs_P[all_data$Assigned==1 & all_data$Treated!=1]/2,
        all_data$hoursTV[all_data$Assigned==1 & all_data$Treated!=1 & all_data$Finished == 1],
        names=c('After', 'Before', 'After', 'Before'), main='Hours Watched (Treatment) per Week', xlab='Hours of TV Watched', ylab="Compliers            Non-Compliers",
        ylim=c(0,40), horizontal=T)
```

**Randomization Inference**

Next, we examine our treatment effect (reported weekly TV hours during survey - pre-survey weekly TV hours) in a more rigorous fashion using randomization inference. We find that for our ATE of -0.95 hrs/week, there is a high p-value (p=0.29). This suggests that while our experiments yields a negative point estimate of the treatment effect (those in the treatment group watched less TV than their control group counterparts during the two weeks of the experiment), our results are not statistically significant and could have occured by chance.

```{r echo=FALSE}
#extract needed columns and put into a data table. create a 'dnd' column for TV watched during survey - pre-survey reported TV hours
cols <- c(1, 9, 22, 29)
dt <- data.table(all_data[ ,cols])
dt [ , dnd := reportedhrs_P/2 - hoursTV] #converting both to weekly #

#use a function to perform the randomization inference
set.seed(131)
ri <- function(iter=500) {
  true_ate <- dt[ , .(m = mean(dnd, na.rm=T)), keyby = Assigned][ , .(ate = diff(m))]
  
  ri_dist <- rep(NA, iter)
    for(i in 1:iter) {
        ri_dist[i] <- dt[ , .(m=mean(dnd, na.rm=T)),keyby=sample(Assigned, replace=T)] %>%
            .[ , .(ri_ate=diff(m))]
    }
    return(list(true_ate, ri_dist))
}

ri_model <- ri(iter=500)

#plot results and calculate p-value
null_dist <- unlist(ri_model[[2]])
hist(null_dist, xlim = c(-10, 10), main= "Sharp Null Distribution", xlab = "Draws from Randomization Inference", col = "black")
abline(v=ri_model[[1]], col = "red")
text(0,80, label=paste0("ATE= ", round(ri_model[[1]],2)), col = "red")
text(7,40, label=paste0("p-value= ",mean(ri_model[[1]]$ate > null_dist)),col = "blue")

```

**Regression Including Covariates**

Next, we analyze the outcome of weekly hours TV watched during the experiment period using covariates to reduce our standard errors (including weekly hours of TV prior to the experiment). Our first step in this process is to identify which of our covariates are interesting to include in a regression model.

A few things stood out to us in terms of high level trends. First of all, the pre-survey "TV hours watched" covariate appeared most strongly correlated with tendency to binge (ie. those who indicated a greater tendency to binge ended up logging more pre-survey hours), tendency to watch TV alone (vs. with someone else or a group), and feelings of regret or that they had spent more time watching TV than they would have liked. In terms of the key outcome measure of reported hours of TV watched during the survey period, as we suspected the correlation matrix below seems to suggest there is a strong positive correlation between pre-survey TV watching and survey TV watching. We also again see a reasonably high positive correlation between the outcome measure and the tendency to binge. 

```{r echo=FALSE}
#reordering some of the factor variables to make more intuitive sense for EDA
all_data$employment <- factor(all_data$employment, levels(all_data$employment)[c(3,5,4,2,6,1)])
all_data$binge <- factor(all_data$binge, levels(all_data$binge)[c(5,4,3,2,1)])
all_data$children <- factor(all_data$children, levels(all_data$children)[c(2,1)])
all_data$shareProfile <- factor(all_data$shareProfile, levels(all_data$shareProfile)[c(2,1)])
all_data$watchAlone <- factor(all_data$watchAlone, levels(all_data$watchAlone)[c(2,1)])
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Make correlation plot
cols <- c(3:15, 20, 22, 30)
all_data_numeric <- all_data[all_data$Finished==1, cols]
colnames(all_data_numeric) =c("gender", "age", "region", "employment", "martial status", "children", "hoursTV", "binge", "primary source", "allMethods", "regrets", "alone", "share profile", "source", "reported hrs", "netflix reading")
DF <- data.frame(lapply(all_data_numeric,as.numeric))
corrmatrix = round(cor(DF[, sapply(DF, is.numeric)], use = "na.or.complete"),
2)
corrplot(corrmatrix)
```

We conducted further EDA for other variables like age, region, and marital status to see if there were any noteworthy patterns. We immediately ruled out age and employment because there were some pretty big clusters in the data that made it hard to get sizable groups (for example, more than 75% of our participants were in the 22-34 age and around 60% of participants had full time jobs). By including them in the equation we'd likely end up with some pretty small groups which could skew the comparison quite a bit. Further, most of our participants were from the Northeast and West Coast which wasn't a huge surprise to us given how we targeted participants but the average TV_hours watched during the survey period by Northeast vs. West Coast doesn't appear to be all that different upon exploring the data. As such, we decided not to include region in the model.   

```{r echo=FALSE}
#employment and age variables
table(all_data$employment[all_data$Finished==1])

table(all_data$age[all_data$Finished==1])
```

```{r fig.height = 3, fig.width = 6, echo=FALSE}
#Region variable
regions <- data.frame(aggregate(all_data$reportedhrs_P[all_data$Finished==1]/2, 
                                by=list(all_data$region[all_data$Finished==1]), FUN=mean, na.rm=TRUE))
colnames(regions) <- c("region", "average_TV_hours")

ggplot(data=regions, aes(x=region, y=average_TV_hours))+
  geom_bar(stat="identity") + ggtitle("Weekly TV by Region")

table(all_data$region[all_data$Finished==1])
```
  
Marital status and gender were two variables that seemed like it could make some sense to include in model. There are roughly an equal proportion of single vs. married individuals in our study (41 vs. 39) and married individuals in the study on average watched a little over 1.5 hours per week more of television during the 2 week study than their single counterparts did. It may make sense to use an indicator variable for 'married' as a result. There were also some modest signs to suggest that females had much more clustered values toward the low end of the TV watching spectrum while most of the outliers were male, making that a potentially interesting variable to include.  

```{r fig.height = 3, echo=FALSE}
#Marital Status variable
all_data$married = ifelse(all_data$maritalStatus == "married", 1, 0)
married <- data.frame(aggregate(all_data$reportedhrs_P[all_data$Finished==1]/2, 
                                by=list(all_data$married[all_data$Finished==1]), FUN=mean, na.rm=TRUE))
colnames(married) <- c("married", "average_TV_hours")

ggplot(data=married, aes(x=factor(married), y=average_TV_hours))+
  geom_bar(stat="identity") + ggtitle("Weekly TV by Marital Status")

```


```{r fig.height=3, echo=FALSE, message=FALSE, warning=FALSE}
# gender variable
ggplot(all_data[all_data$Finished==1,]) +
  geom_density(aes(x=all_data$reportedhrs_P[all_data$Finished==1]/2, fill=all_data$gender[all_data$Finished==1])) +
  ggtitle("Weekly TV by Gender")
```


Finally, we examined whether the participant provided their data from the Netflix extension in the pre-survey. While the end data from this extension was not useful for analysis, we were concerned that seeing this data may have an impact on the subsequent amount of TV a participant watched (although the randomization means this would not bias our results). Indeed, we see that on average, those who reported the times from their Netflix extension watched about 2 hours less TV per week. Note that this is not a causal estimate since it could be a selection effect (those who chose to install the extension are those who watched less TV to begin with). Still, this is a variable to try including as a covariate as it may help lower our standard errors.

```{r fig.height=3, echo=FALSE, message=FALSE, warning=FALSE}
# Provided Netflix extension data in pre-survey variable
extension <- data.frame(aggregate(all_data$reportedhrs_P[all_data$Finished==1]/2, 
                                by=list(all_data$extension[all_data$Finished==1]), FUN=mean, na.rm=TRUE))
colnames(extension) <- c("included_extension_results", "average_TV_hours")

ggplot(data=extension, aes(x=included_extension_results, y=average_TV_hours))+
  geom_bar(stat="identity") + ggtitle("Weekly TV by Reporting Netflix Extension Times")

```

Having explored our data and identified several potential covariates, we now run our regression models. This includes a base model with only the pre-experiment hours of TV as a covariate, an intermediate model with the additional binary covariates, then also the full covariate model including all variables of interest. We see that for all the models, our estimate of the ATE is negative but not statistically significant (ranges from -0.2 to -0.6). We also see that the only statistically significant variable (at a p-value of 5% or less) was the pre-survey TV hours. This indicates that the other potential covariates do not explain more of the variance in our outcome TV watched beyond that already accounted for in the pre-survey TV hours.

```{r results='asis', echo=FALSE, message=FALSE}
mod_base <- lm(reportedhrs_P/2 ~ Assigned + hoursTV, data=all_data)
mod_int <- lm(reportedhrs_P/2 ~ Assigned + hoursTV + watchAlone + married + gender + extension, data=all_data)
mod_EDA <- lm(reportedhrs_P/2 ~ Assigned + hoursTV +  binge + moreTimeThanWanted + watchAlone + married + gender + extension, data=all_data)
stargazer(mod_base, mod_int, mod_EDA, header=F, no.space=TRUE)
```

**Regression Heterogeneous Effects**

Above, we identified above that our pre-survey amount of TV watched was the only statistically significant variable in predicting the amount of TV watched during the experiment. Combining this with our original hypothesis that those watching less TV would not have the same treatment effect, we will test for a heterogenous effect based on pre-survey hours of TV watched. We test this in two ways - pure pre-survey hours TV, and a dummy variable for participants being in the first quartile.

We again find that our treatment effect and heterogeneous treatment effect are not statistically significant in either specification. The point estimates align with our expectations. In the first model on pure hours TV, we see larger treatment effects when the participant watched more hours of TV before the experiment. In the second model using the dummy variable, we see that for those in the first quartile who would presumably not feel any peer pressure to watch less TV based on the information we provided, the treatment effect is practically 0 (-0.8 + 0.746), compared to the treatment effect for those not in the first quartile of -0.8.

```{r results='asis', echo=FALSE, message=FALSE}
mod_h1 <- lm(reportedhrs_P/2 ~ Assigned + hoursTV + Assigned:hoursTV, data=all_data)
mod_h2 <- lm(reportedhrs_P/2 ~ Assigned + hoursTV + first_quartile + Assigned:first_quartile, data=all_data)
stargazer(mod_h1, mod_h2, header=F, no.space=TRUE)
```


##Conclusion

Our study on TV watching behavior was ultimately inconclusive as there were no statistical differences in actual watching behavior for the group assigned to the treatment of being shown how much TV peers are watching. However, while the treatment effect was not statistically significant, we were able to show a consistent negative point estimate of the treatment effect across multiple specifications in the range of 0.2 - 1 hour / week, and up to -2.2 hours/week for CACE (assuming our manipulation check truly identified compliance). We also found point estimates of the heterogeneous treatment effects which agreed with our original hypothesis, where those in the first quartile did not have a treatment effect and those not in the first quartile had a stronger negative effect of -0.8. The most statistically significant predictor we found was previous TV watching behavior. We believe that our study was likely underpowered to identify these small treatment effects with significance.

That said, we would question the generalizability of this study. Given our limited resources, we sampled from a pretty narrow constituent base. Most of our participants were aged 22-34 and living in either the Northeast or West Coast, with a relatively large student contingent. This sample is likely not that indicative of the broader population and an experiment with a larger sample size and more representative population would have to be conducted to learn more.

##Appendix

**The pre-survey in the pilot consisted of the following questions:**

>1. Provide us with an email address that will be used to contact you throughout the course of this survey.
>2. Are you male or female?
>3. How old are you?
>4. What region do you currently live in?
>5. What is your current employment status?
>6. What is your marital status?
>7. Do you have children under the age of 18?
>8. How much television do you watch on a weekly basis? This includes cable, online streaming, Youtube, and can consist of movies, tv shows, and video clips.
>9. Do you tend to binge watch TV shows (more than 4 hours of consecutive TV watching)?
>10. What is your primary method of watching television?
>11. Check off all the ways in which you consume television - Netflix, HBO, Hulu, Amazon, Youtube, Cable, Other.
>12. How often do you find that after watching television you spent more time on it than you would have liked to?
>13. Do you tend to watch television alone or with others?
>14. Do you share your Netflix profile with someone else?
>15. A link to installing the Netflix Chrome instruction as well as a text box for inputting what the extension outputs as usage over the last two week. 

**The post-survey in the pilot consisted of the following questions:**  

>1. Provide us with your email address. This is the same identifier you used on the first survey and how we have been contacting you.
>2. How much television did you watch during the study period from Feb 28 through March 1, inclusive? This includes cable, online streaming, and YouTube, and can consist of movies, tv shows, and video clips.
>3. Input the time reading from your Chrome Netflix extension.
>4. (treatment group only) Did you receive an email between surveys providing a bar chart with information about TV watching habits among our study participants?

\newpage

**Summary of pilot pre-survey takeaways:**  

1. 8 of 9 individuals filled out the pre-survey in the designated time. Given the quick turnaround time (2 days), we expected some level of non-response as work, travel, etc can get in the way. 

2. of the 8 individuals who did respond, 5 were Female and 3 male; 7 of the 8 were in the 22-34 age group, 7 of the 8 were in the Northeast region, 7 of 8 were full-time employed; 6 of 8 were married and 2 were single; and none of the 8 individuals had children under the age of 18.

3. At the mid-point of the ranges, individuals in the pilot reported watching 7.75 hours of TV a week. Netflix was the primary method of watching TV for half of the individuals while cable and other were the primary method for the other half while 7 of the 8 individuals said they consumed television through Netflix. 

4. 3 of 8 individuals were unable to install the extension either because they don't have a Netflix account or they don't use Chrome as their web browser. Of the individuals that did input a reading from the extension, the average Netflix consumption over the prior two weeks was 787.2 minutes or just over 13 hours of TV.

**The experiment timeline**   

>Mar 8-11 - gather participants  
>Mar 12 - send out pre-survey early in the morning  
>Mar 15 - close pre-survey, randomize intervention, and covariance imbalance check  
>Mar 18 - treatment/control email [completed 11pm EST]  
>Apr 2 - start final data collection  
>Apr 6 - end final data collection  
>Apr 7 - start data analysis 

**Covariate balance check model results**

```{r results='asis', echo=FALSE}
stargazer(cov_balance_check1, attrit_covariate_check, 
          column.labels=c("Initial", "Post-attrition"),
          report="vc*", header=F, no.space=TRUE)
```

\newpage
  
```{r echo=FALSE, fig.cap="Standardized pitch", out.width = '100%'}
knitr::include_graphics("pitch.png")
```

```{r echo=FALSE, fig.cap="Flow Diagram for Participants", out.width = '100%'}
knitr::include_graphics("flow.png")
```

\newpage

```{r echo=FALSE, fig.cap="Sample Control email", out.width = '100%'}
knitr::include_graphics("control.png")
```
  
```{r echo=FALSE, fig.cap="Sample Treatment email", out.width = '100%'}
knitr::include_graphics("treatment.png")
```